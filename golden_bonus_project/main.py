# main.py
import streamlit as st
from nodes.advisor import AdvisorNode
from core.pipeline import Pipeline
from config.settings import PAGE_TITLE, PAGE_HEADER, PIPELINE_CACHE_VERSION

# 1. é é¢è¨­å®š
st.set_page_config(page_title=PAGE_TITLE, layout="wide")
st.title(PAGE_HEADER)

# å´é‚Šæ¬„ï¼šAI é€£ç·šç‹€æ…‹ï¼ˆä¸é¡¯ç¤ºæ•æ„Ÿè³‡è¨Šï¼‰
with st.sidebar:
    st.markdown("### ğŸ”Œ AI é€£ç·šç‹€æ…‹")
    try:
        from utils.gemini_client import get_api_key_source, test_gemini_connection

        key_source = get_api_key_source()
        st.caption(f"Key ä¾†æºï¼š{key_source or 'æœªè¨­å®š'}")

        if st.button("æ¸¬è©¦ Gemini é€£ç·š", use_container_width=True):
            ok, msg = test_gemini_connection()
            if ok:
                st.success(msg)
            else:
                st.error(msg)

        if st.button("æ¸…é™¤å¿«å– / é‡å»º Pipeline", use_container_width=True):
            st.cache_resource.clear()
            st.rerun()
    except Exception as e:
        st.warning(f"ç„¡æ³•è¼‰å…¥é€£ç·šæª¢æŸ¥ï¼š{str(e)}")

# 2. ç‹€æ…‹åˆå§‹åŒ–
if "messages" not in st.session_state:
    st.session_state.messages = []  # ç”¨ä¾†å­˜å°è©±æ­·å²

# 3. åˆå§‹åŒ– Pipelineï¼ˆåªåŒ…å« AdvisorNodeï¼‰
@st.cache_resource
def get_pipeline(_cache_version: str):
    pipe = Pipeline()
    pipe.add_node(AdvisorNode("Advisor"))
    return pipe

pipeline = get_pipeline(PIPELINE_CACHE_VERSION)

# 4. å°è©±æ©Ÿå™¨äººä»‹é¢
st.markdown("---")
st.subheader("ğŸ’¬ å¹´çµ‚çé‡‘é¡§å•å°è©±æ©Ÿå™¨äºº")
st.info("ğŸ’¡ **ä½¿ç”¨æç¤º**ï¼šæ‚¨å¯ä»¥è©¢å•ä»»ä½•é—œæ–¼å¹´çµ‚çé‡‘ç™¼æ”¾ç­–ç•¥çš„å•é¡Œï¼ŒAI é¡§å•æœƒæ ¹æ“šå°ˆæ¥­çŸ¥è­˜åº«ç‚ºæ‚¨æä¾›å»ºè­°ã€‚")

# é¡¯ç¤ºæ­·å²å°è©±
for message in st.session_state.messages:
    with st.chat_message(message["role"], avatar="ğŸ¤–" if message["role"] == "assistant" else "ğŸ‘¤"):
        st.markdown(message["content"])

# è™•ç†ç”¨æˆ¶è¼¸å…¥
if prompt := st.chat_input("è«‹è¼¸å…¥æ‚¨çš„å•é¡Œæˆ–æ˜¯è²¼ä¸Šåƒè€ƒè³‡è¨Š... (ä¾‹å¦‚ï¼šå…¬å¸å ±å‘Šã€å•å·çµæœã€è¨è«–ç´€éŒ„ç­‰)"):
    # 1. å°‡ç”¨æˆ¶è¨Šæ¯åŠ å…¥å°è©±æ­·å²
    st.session_state.messages.append({"role": "user", "content": prompt})
    
    # 2. é¡¯ç¤ºç”¨æˆ¶è¨Šæ¯
    with st.chat_message("user"):
        st.markdown(prompt)
    
    # 3. æº–å‚™èŠå¤©ç”¨çš„ context
    chat_context = {
        "current_intent": "CHAT",
        "latest_user_question": prompt,
        "history": [
            {"role": msg["role"], "content": msg["content"]}
            for msg in st.session_state.messages[:-1]  # æ’é™¤æœ€å¾Œä¸€æ¢ï¼ˆå‰›åŠ å…¥çš„ç”¨æˆ¶è¨Šæ¯ï¼‰
        ]
    }
    
    # 4. åŸ·è¡Œ AdvisorNode
    with st.chat_message("assistant", avatar="ğŸ¤–"):
        with st.spinner("AI æ€è€ƒä¸­..."):
            try:
                # åŸ·è¡ŒèŠå¤© Pipeline
                result_context = pipeline.run(chat_context)
                
                # 5. é¡¯ç¤º AI å›æ‡‰
                ai_response = result_context.get("ai_response", "æŠ±æ­‰ï¼Œæˆ‘ç„¡æ³•å›ç­”é€™å€‹å•é¡Œã€‚")
                st.markdown(ai_response)
                
                # 6. å°‡ AI å›æ‡‰åŠ å…¥å°è©±æ­·å²
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": ai_response
                })
                
            except Exception as e:
                error_msg = f"âš ï¸ ç³»çµ±éŒ¯èª¤ï¼š{str(e)}"
                st.error(error_msg)
                st.session_state.messages.append({
                    "role": "assistant",
                    "content": error_msg
                })
